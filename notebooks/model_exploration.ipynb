{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Language Model Exploration\n",
    "\n",
    "This notebook demonstrates how to build and train a custom language model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data\n",
    "\n",
    "First, let's load some sample text data to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample text for demonstration\n",
    "# In a real project, you would load data from files\n",
    "sample_text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n",
    "concerned with the interactions between computers and human language, in particular how to program computers \n",
    "to process and analyze large amounts of natural language data. The goal is a computer capable of understanding \n",
    "the contents of documents, including the contextual nuances of the language within them.\n",
    "\n",
    "The technology can then accurately extract information and insights contained in the documents as well as \n",
    "categorize and organize the documents themselves.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Sample text length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Let's create a simple character-level tokenizer for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.data.tokenizer import CharacterTokenizer\n",
    "\n",
    "# Create and train the tokenizer\n",
    "tokenizer = CharacterTokenizer()\n",
    "tokenizer.train([sample_text])\n",
    "\n",
    "# Encode the text\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(f\"Encoded length: {len(encoded)}\")\n",
    "print(f\"First 20 tokens: {encoded[:20]}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"\\nDecoded sample: {decoded[:50]}...\")\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.token_to_id)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.data.dataset import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a dataset\n",
    "block_size = 64  # Context length\n",
    "dataset = TextDataset([encoded], block_size=block_size)\n",
    "\n",
    "# Create a dataloader\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Inspect a batch\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "\n",
    "# Show a sample\n",
    "sample_idx = 0\n",
    "input_text = tokenizer.decode(batch['input_ids'][sample_idx].tolist())\n",
    "target_text = tokenizer.decode(batch['labels'][sample_idx].tolist())\n",
    "\n",
    "print(f\"\\nInput: {input_text[:30]}...\")\n",
    "print(f\"Target: {target_text[:30]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Model\n",
    "\n",
    "Now let's create our transformer-based language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from src.model.transformer import CustomLanguageModel\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 128       # Embedding dimension\n",
    "num_heads = 4       # Number of attention heads\n",
    "num_layers = 3      # Number of transformer layers\n",
    "d_ff = 512          # Feed-forward dimension\n",
    "dropout = 0.1       # Dropout rate\n",
    "\n",
    "# Create the model\n",
    "model = CustomLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=block_size,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Test forward pass\n",
    "input_ids = batch['input_ids']\n",
    "outputs = model(input_ids)\n",
    "print(f\"Output shape: {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "from src.training.trainer import Trainer\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Create learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    checkpoint_dir=\"../checkpoints\",\n",
    "    use_wandb=False  # Set to True if you want to use Weights & Biases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Let's train our model for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train for a few epochs\n",
    "num_epochs = 5\n",
    "trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Text\n",
    "\n",
    "Now let's use our trained model to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prepare a prompt\n",
    "prompt = \"Natural language\"\n",
    "prompt_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long)\n",
    "\n",
    "# Generate text\n",
    "max_length = 100\n",
    "temperature = 0.8\n",
    "generated_ids = model.generate(prompt_ids, max_length=max_length, temperature=temperature)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(f\"Generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "Let's calculate perplexity on our sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            labels = labels.view(-1)\n",
    "            \n",
    "            # Calculate cross entropy loss\n",
    "            loss = F.cross_entropy(outputs, labels, ignore_index=0, reduction=\"sum\")\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (labels != 0).sum().item()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = calculate_perplexity(model, dataloader)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"../checkpoints/character_model.pt\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"../checkpoints/character_tokenizer.json\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "Here are some ways to improve the model:\n",
    "\n",
    "1. Train on a larger dataset\n",
    "2. Use a more sophisticated tokenizer (BPE, WordPiece)\n",
    "3. Increase model size (more layers, larger embedding dimension)\n",
    "4. Implement techniques like flash attention for efficiency\n",
    "5. Add regularization techniques\n",
    "6. Experiment with different architectures (GPT, BERT, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
